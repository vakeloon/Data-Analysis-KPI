{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-05T13:04:34.469481Z",
     "start_time": "2025-01-05T13:03:46.643180Z"
    }
   },
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline('sentiment-analysis')"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "D:\\маг1курс\\ІАД\\lab4\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\111\\.cache\\huggingface\\hub\\models--distilbert--distilbert-base-uncased-finetuned-sst-2-english. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\маг1курс\\ІАД\\lab4\\.venv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\маг1курс\\ІАД\\lab4\\.venv\\Lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n",
      "Device set to use 0\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-05T13:15:04.708122Z",
     "start_time": "2025-01-05T13:15:04.475858Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Класифікація тексту по сенсовому забарвленню (positive or negative)\n",
    "txt_1 = 'Football is the most exciting and popular game in the world'\n",
    "classifier(txt_1)"
   ],
   "id": "5ccd5a817095b5dc",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998573064804077}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-05T13:05:37.849821Z",
     "start_time": "2025-01-05T13:05:37.636652Z"
    }
   },
   "cell_type": "code",
   "source": [
    "txt_2 = 'I am very disappointed with the results.'\n",
    "classifier(txt_2)"
   ],
   "id": "c90520a470d8206e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9997926354408264}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-05T13:16:31.206331Z",
     "start_time": "2025-01-05T13:15:56.522246Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ],
   "id": "b8fd9b75d5c1d8aa",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\маг1курс\\ІАД\\lab4\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\111\\.cache\\huggingface\\hub\\models--distilbert-base-uncased-finetuned-sst-2-english. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-05T13:18:14.057403Z",
     "start_time": "2025-01-05T13:18:14.029243Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Дефолтна токенізація\n",
    "inputs = tokenizer([txt_1, txt_2])\n",
    "inputs"
   ],
   "id": "a90f3b051e72bbcf",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 2374, 2003, 1996, 2087, 10990, 1998, 2759, 2208, 1999, 1996, 2088, 102], [101, 1045, 2572, 2200, 9364, 2007, 1996, 3463, 1012, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-05T13:20:07.948263Z",
     "start_time": "2025-01-05T13:20:07.928016Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Токенізація з параметрами\n",
    "inputs_with_padding = tokenizer([txt_1, txt_2], padding = True, truncation = True, max_length = 256, return_tensors=\"tf\")\n",
    "inputs_with_padding"
   ],
   "id": "6197d7d607f2f650",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(2, 13), dtype=int32, numpy=\n",
       "array([[  101,  2374,  2003,  1996,  2087, 10990,  1998,  2759,  2208,\n",
       "         1999,  1996,  2088,   102],\n",
       "       [  101,  1045,  2572,  2200,  9364,  2007,  1996,  3463,  1012,\n",
       "          102,     0,     0,     0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(2, 13), dtype=int32, numpy=\n",
       "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]], dtype=int32)>}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-05T13:23:22.409956Z",
     "start_time": "2025-01-05T13:23:22.181753Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Пропускання токенізованого тексту через модель\n",
    "outputs = model(inputs_with_padding)\n",
    "outputs"
   ],
   "id": "b59701acdcadb02a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TFSequenceClassifierOutput(loss=None, logits=<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[-4.24948  ,  4.605344 ],\n",
       "       [ 4.707576 , -3.7732737]], dtype=float32)>, hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-05T13:22:24.037606Z",
     "start_time": "2025-01-05T13:22:24.015367Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Ймовірності забарвлення тексту (стовпці 0 - negative, 1 - positive)\n",
    "import tensorflow as tf\n",
    "predictions = tf.nn.softmax(outputs[0], axis=-1)\n",
    "predictions"
   ],
   "id": "2e31a8f28f6bb655",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[1.4267136e-04, 9.9985731e-01],\n",
       "       [9.9979264e-01, 2.0735948e-04]], dtype=float32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-05T14:19:16.587594Z",
     "start_time": "2025-01-05T14:19:11.048341Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Україномовна модель для визначення в тексті слів, що означають локацію, людину або організацію\n",
    "from transformers import TFAutoModelForTokenClassification\n",
    "\n",
    "model_name = \"ukr-models/uk-ner\" \n",
    "model = TFAutoModelForTokenClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
   ],
   "id": "2578696c108869d2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFXLMRobertaForTokenClassification: ['roberta.embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFXLMRobertaForTokenClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFXLMRobertaForTokenClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFXLMRobertaForTokenClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFXLMRobertaForTokenClassification for predictions without further training.\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-05T14:19:44.455287Z",
     "start_time": "2025-01-05T14:19:44.427536Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "import tokenize_uk\n",
    "\n",
    "def get_word_predictions(model, tokenizer, texts):\n",
    "    results = []\n",
    "\n",
    "    for text in texts:\n",
    "        tokens = tokenize_uk.tokenize_text(text)\n",
    "        inputs = tokenizer(text, return_tensors=\"tf\", padding=True, truncation=True)\n",
    "        \n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits  \n",
    "        predictions = tf.argmax(logits, axis=-1)  \n",
    "\n",
    "        word_predictions = []\n",
    "        tokens_index = 0 \n",
    "\n",
    "        for i, token in enumerate(inputs['input_ids'][0]):\n",
    "            if token != tokenizer.pad_token_id: \n",
    "                label_id = predictions[0][tokens_index].numpy()  \n",
    "                label = model.config.id2label[label_id]  \n",
    "                word_predictions.append({\"word\": tokenizer.decode([token]), \"label\": label})\n",
    "                tokens_index += 1\n",
    "\n",
    "        results.append(word_predictions)\n",
    "\n",
    "    return results"
   ],
   "id": "97dc2041a52b9e21",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-05T14:28:25.654765Z",
     "start_time": "2025-01-05T14:28:25.642116Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def visualize_predictions(predictions):\n",
    "    label_dict = {\n",
    "        'B-PER': 'Person',\n",
    "        'I-PER': 'Person',\n",
    "        'B-LOC': 'Location',\n",
    "        'I-LOC': 'Location',\n",
    "        'B-ORG': 'Organization',\n",
    "        'I-ORG': 'Organization',\n",
    "        'O': 'Other'\n",
    "    }\n",
    "    \n",
    "    print(f\"{'Word':<15}{'Label'}\")\n",
    "    print(\"-\" * 30)  \n",
    "\n",
    "    for sentence in predictions:\n",
    "        for token_info in sentence:\n",
    "            word = token_info['word']\n",
    "            label = token_info['label']\n",
    "            if label != 'O':\n",
    "                label_value = label_dict.get(label, label)  \n",
    "                print(f\"{word:<15}{label_value}\")"
   ],
   "id": "4daeb5c9816f6ea1",
   "outputs": [],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-05T14:28:28.521441Z",
     "start_time": "2025-01-05T14:28:27.869610Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Тестування на прикладі\n",
    "texts = [\"Микола Лисенко був видатним українським композитором. Він народився в Києві, але частину свого життя провів у Львові. У Києві він заснував музичну школу, а також працював в оперному театрі. Лисенко співпрацював з багатьма художніми колективами, серед яких був Національний ансамбль пісні та танцю України.\"]\n",
    "predictions = get_word_predictions(model, tokenizer, texts)\n",
    "print(predictions)"
   ],
   "id": "4f3da29671a49aa1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'word': '<s>', 'label': 'O'}, {'word': 'Микола', 'label': 'B-PER'}, {'word': 'Лис', 'label': 'I-PER'}, {'word': 'енко', 'label': 'I-PER'}, {'word': 'був', 'label': 'O'}, {'word': 'видат', 'label': 'O'}, {'word': 'ним', 'label': 'O'}, {'word': 'українським', 'label': 'O'}, {'word': 'композитор', 'label': 'O'}, {'word': 'ом', 'label': 'O'}, {'word': '.', 'label': 'O'}, {'word': 'Він', 'label': 'O'}, {'word': 'народився', 'label': 'O'}, {'word': 'в', 'label': 'O'}, {'word': 'Києві', 'label': 'B-LOC'}, {'word': ',', 'label': 'O'}, {'word': 'але', 'label': 'O'}, {'word': 'частину', 'label': 'O'}, {'word': 'свого', 'label': 'O'}, {'word': 'життя', 'label': 'O'}, {'word': 'пров', 'label': 'O'}, {'word': 'ів', 'label': 'O'}, {'word': 'у', 'label': 'O'}, {'word': 'Львові', 'label': 'B-LOC'}, {'word': '.', 'label': 'O'}, {'word': 'У', 'label': 'O'}, {'word': 'Києві', 'label': 'B-LOC'}, {'word': 'він', 'label': 'O'}, {'word': 'зас', 'label': 'O'}, {'word': 'н', 'label': 'O'}, {'word': 'ував', 'label': 'O'}, {'word': 'муз', 'label': 'O'}, {'word': 'и', 'label': 'O'}, {'word': 'чну', 'label': 'O'}, {'word': 'школу', 'label': 'O'}, {'word': ',', 'label': 'O'}, {'word': 'а', 'label': 'O'}, {'word': 'також', 'label': 'O'}, {'word': '', 'label': 'O'}, {'word': 'працював', 'label': 'O'}, {'word': 'в', 'label': 'O'}, {'word': 'опер', 'label': 'O'}, {'word': 'ному', 'label': 'O'}, {'word': 'театр', 'label': 'O'}, {'word': 'і', 'label': 'O'}, {'word': '.', 'label': 'O'}, {'word': 'Лис', 'label': 'B-PER'}, {'word': 'енко', 'label': 'B-PER'}, {'word': 'спів', 'label': 'O'}, {'word': 'працював', 'label': 'O'}, {'word': 'з', 'label': 'O'}, {'word': 'бага', 'label': 'O'}, {'word': 'ть', 'label': 'O'}, {'word': 'ма', 'label': 'O'}, {'word': 'художн', 'label': 'O'}, {'word': 'і', 'label': 'O'}, {'word': 'ми', 'label': 'O'}, {'word': 'колектив', 'label': 'O'}, {'word': 'ами', 'label': 'O'}, {'word': ',', 'label': 'O'}, {'word': 'серед', 'label': 'O'}, {'word': 'яких', 'label': 'O'}, {'word': 'був', 'label': 'O'}, {'word': 'Національний', 'label': 'B-ORG'}, {'word': 'ансамбл', 'label': 'I-ORG'}, {'word': 'ь', 'label': 'I-ORG'}, {'word': 'пісні', 'label': 'I-ORG'}, {'word': 'та', 'label': 'I-ORG'}, {'word': 'танц', 'label': 'I-ORG'}, {'word': 'ю', 'label': 'I-ORG'}, {'word': 'України', 'label': 'I-ORG'}, {'word': '.', 'label': 'O'}, {'word': '</s>', 'label': 'O'}]]\n"
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-05T14:28:30.861202Z",
     "start_time": "2025-01-05T14:28:30.853525Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Видає лише визначені слова\n",
    "visualize_predictions(predictions)"
   ],
   "id": "7029df032790afd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word           Label\n",
      "------------------------------\n",
      "Микола         Person\n",
      "Лис            Person\n",
      "енко           Person\n",
      "Києві          Location\n",
      "Львові         Location\n",
      "Києві          Location\n",
      "Лис            Person\n",
      "енко           Person\n",
      "Національний   Organization\n",
      "ансамбл        Organization\n",
      "ь              Organization\n",
      "пісні          Organization\n",
      "та             Organization\n",
      "танц           Organization\n",
      "ю              Organization\n",
      "України        Organization\n"
     ]
    }
   ],
   "execution_count": 56
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
